{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.listings import *\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "import pickle5 as pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os.path\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace URL to get the list of advertisments\n",
    "details_dict.clear()\n",
    "details_dict = get_list(\"https://www.wg-gesucht.de/wg-zimmer-in-Passau.104.0.1.3.html\")\n",
    "print(details_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract page and save a local copy of HTML\n",
    "def extract(URL):\n",
    "    print(URL)\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    sleep(randint(2,10))\n",
    "    \n",
    "    #Download website locally (Replace the path with where the folder is located on your system)\n",
    "    file_name = '/Users/ummerabab-/Desktop/wg-gesucht/Websites/' + URL.split(\"https://www.wg-gesucht.de/\")[1].split('.html')[0] + '.txt'\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        file.write(page.content)\n",
    "            \n",
    "    return soup\n",
    "\n",
    "# Acquire useful information from HTML tags\n",
    "def transform(soup):\n",
    "    panel_body = soup.find('div', class_ = 'panel-body')\n",
    "    rows = panel_body.find_all('div', class_ = 'row')\n",
    "    \n",
    "    wg_tag_str = ''\n",
    "    \n",
    "    for item in rows:\n",
    "        #WG-Details\n",
    "        for sub_item in item.find_all('div', class_ = 'col-sm-6'):\n",
    "            if sub_item.h4.text.strip() == 'Die WG':\n",
    "                ul_tag = sub_item.find('ul')\n",
    "                for li in ul_tag.find_all('li'):\n",
    "                    wg_tag_str = wg_tag_str + ';' + re.sub('\\s+',' ',(li.text.strip().replace('\\n','')))\n",
    "                wg_tag.append(wg_tag_str)\n",
    " \n",
    "            if sub_item.h4.text.strip() == 'Gesucht wird':\n",
    "                ul_tag = sub_item.find('ul')\n",
    "                for li in ul_tag.find_all('li'):\n",
    "                    gesucht_wird.append(re.sub('\\s+',' ',(li.text.strip().replace('\\n',''))))\n",
    "                    \n",
    "        #Address\n",
    "        if item.find('a', href = '#mapContainer'):\n",
    "            adresse_str = re.sub('\\s+',' ',(item.find('a', href = '#mapContainer').text.strip().replace('\\n','')))\n",
    "            adresse.append(adresse_str)\n",
    "    \n",
    "         #Person\n",
    "#        if item.find('div', class_ = 'mb10'):\n",
    "#            if item.find('img'):\n",
    "#                person_img_src = item.find('img')['src']\n",
    "#                 print(person_img_src['src'])\n",
    "\n",
    "\n",
    "    \n",
    "    #Description\n",
    "    description_tag = panel_body.find('div', id = 'ad_description_text')\n",
    "    #print(description_tag)\n",
    "\n",
    "#     zimmer = ''\n",
    "#     lage = ''\n",
    "#     wg_leben = ''\n",
    "#     sonstiges = ''\n",
    "        \n",
    "    if description_tag.find('div', id ='freitext_0'):\n",
    "            zimmer.append(description_tag.find('p', id ='freitext_0_content').text.strip().replace('\\n','').replace('\\r',''))\n",
    "    else:\n",
    "            zimmer.append('')\n",
    "    if description_tag.find('div', id ='freitext_1'):\n",
    "            lage.append(description_tag.find('p', id ='freitext_1_content').text.strip().replace('\\n','').replace('\\r',''))\n",
    "    else:\n",
    "            lage.append('')\n",
    "    if description_tag.find('div', id ='freitext_2'):\n",
    "            wg_leben.append(description_tag.find('p', id ='freitext_2_content').text.strip().replace('\\n','').replace('\\r',''))\n",
    "    else:\n",
    "            wg_leben.append('')\n",
    "    if description_tag.find('div', id ='freitext_3'):\n",
    "            sonstiges.append(description_tag.find('p', id ='freitext_3_content').text.strip().replace('\\n','').replace('\\r',''))\n",
    "    else:\n",
    "            sonstiges.append('')\n",
    "            \n",
    "\n",
    "    # Set values in details_dict\n",
    "    details_dict['Die_WG'] = wg_tag\n",
    "    details_dict['Gesucht_wird'] = gesucht_wird\n",
    "    details_dict['Adresse']= adresse\n",
    "    details_dict['Zimmer'] = zimmer\n",
    "    details_dict['Lage'] = lage\n",
    "    details_dict['WG-Leben'] = wg_leben\n",
    "    details_dict['Sonstiges'] = sonstiges\n",
    "\n",
    "    return details_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_array = details_dict['URL']\n",
    "print(url_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_tag = []\n",
    "gesucht_wird = []\n",
    "adresse = []\n",
    "zimmer = []\n",
    "lage = []\n",
    "wg_leben = []\n",
    "sonstiges = []\n",
    "\n",
    "# Loop through 20 advertisments per page\n",
    "for url in url_array:\n",
    "    \n",
    "    #Specify the URL to extract from\n",
    "    page = extract(url)\n",
    "    # Call functions to Extract HTML from URL and Transform it.\n",
    "    details_dict = transform(page)\n",
    "\n",
    "print(details_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(details_dict['Name']))\n",
    "print(len(details_dict['URL']))\n",
    "print(len(details_dict['Title']))\n",
    "print(len(details_dict['Die_WG']))\n",
    "print(len(details_dict['Gesucht_wird']))\n",
    "print(len(details_dict['Adresse']))\n",
    "print(len(details_dict['Zimmer']))\n",
    "print(len(details_dict['WG-Leben']))\n",
    "print(len(details_dict['Sonstiges']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to data_frame\n",
    "#data_frame = pd.DataFrame([details_dict])\n",
    "data_frame = pd.DataFrame.from_dict(details_dict)\n",
    "print (data_frame)\n",
    "\n",
    "# Save to file with pickle in csv format\n",
    "csv_path = '/Users/ummerabab-/Desktop/wg-gesucht/Code/dataset.csv'\n",
    "if(os.path.isfile(csv_path)):\n",
    "    data_frame.to_csv(csv_path, mode='a', encoding='utf-8', index= False, header=False) #\n",
    "else:\n",
    "    data_frame.to_csv(csv_path, encoding='utf-8', index= False)\n",
    "#data_frame.to_csv(r'/Users/ummerabab-/Desktop/wg-gesucht/Code/dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
